package api

import (
	"bufio"
	"fmt"
	"regexp"
	"strings"
	"sublink/models"
	"sublink/utils"
	"sync"
	"time"

	"github.com/gin-gonic/gin"
)

// ConvertRulesRequest è§„åˆ™è½¬æ¢è¯·æ±‚
type ConvertRulesRequest struct {
	RuleSource       string `json:"ruleSource"`       // è¿œç¨‹ ACL é…ç½® URL
	Category         string `json:"category"`         // clash / surge
	Expand           bool   `json:"expand"`           // æ˜¯å¦å±•å¼€è§„åˆ™
	Template         string `json:"template"`         // å½“å‰æ¨¡æ¿å†…å®¹
	UseProxy         bool   `json:"useProxy"`         // æ˜¯å¦ä½¿ç”¨ä»£ç†
	ProxyLink        string `json:"proxyLink"`        // ä»£ç†èŠ‚ç‚¹é“¾æ¥ï¼ˆå¯é€‰ï¼‰
	EnableIncludeAll bool   `json:"enableIncludeAll"` // æ˜¯å¦å¯ç”¨ include-all æ¨¡å¼
}

// ConvertRulesResponse è§„åˆ™è½¬æ¢å“åº”
type ConvertRulesResponse struct {
	Content string `json:"content"` // è½¬æ¢åçš„å®Œæ•´æ¨¡æ¿å†…å®¹
}

// ACLRuleset ACL è§„åˆ™é›†å®šä¹‰
type ACLRuleset struct {
	Group   string // ç›®æ ‡ä»£ç†ç»„
	RuleURL string // è§„åˆ™ URL æˆ–å†…è”è§„åˆ™
}

// ACLProxyGroup ACL ä»£ç†ç»„å®šä¹‰
type ACLProxyGroup struct {
	Name       string   // ç»„å
	Type       string   // ç±»å‹: select, url-test, fallback, load-balance
	Proxies    []string // ä»£ç†åˆ—è¡¨ï¼ˆç­–ç•¥ç»„å¼•ç”¨ï¼‰
	URL        string   // æµ‹é€Ÿ URL (url-test ç±»å‹)
	Interval   int      // æµ‹é€Ÿé—´éš”
	Tolerance  int      // å®¹å·® (url-test ç±»å‹)
	IncludeAll bool     // æ˜¯å¦åŒ…å«æ‰€æœ‰èŠ‚ç‚¹ï¼ˆ.* é€šé…ç¬¦ï¼‰
	Filter     string   // æ­£åˆ™è¿‡æ»¤å™¨ï¼ˆåˆå¹¶åçš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
}

// ConvertRules è§„åˆ™è½¬æ¢ API
func ConvertRules(c *gin.Context) {
	var req ConvertRulesRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		utils.FailWithMsg(c, "å‚æ•°é”™è¯¯: "+err.Error())
		return
	}

	if req.RuleSource == "" {
		utils.FailWithMsg(c, "è¯·æä¾›è¿œç¨‹è§„åˆ™é…ç½®åœ°å€")
		return
	}

	if req.Category == "" {
		req.Category = "clash"
	}

	// æ£€æµ‹æ¨¡æ¿ç±»å‹ä¸é€‰æ‹©çš„ç±»åˆ«æ˜¯å¦åŒ¹é…
	templateType := detectTemplateType(req.Template)
	if templateType != "" && templateType != req.Category {
		utils.FailWithMsg(c, fmt.Sprintf("æ¨¡æ¿å†…å®¹ä¸é€‰æ‹©çš„ç±»åˆ«ä¸åŒ¹é…ï¼šæ£€æµ‹åˆ° %s æ ¼å¼çš„æ¨¡æ¿ï¼Œä½†é€‰æ‹©çš„ç±»åˆ«æ˜¯ %s", templateType, req.Category))
		return
	}

	// å¦‚æœæ¨¡æ¿ä¸ºç©ºï¼Œè‡ªåŠ¨è¡¥å…¨é»˜è®¤å†…å®¹
	if strings.TrimSpace(req.Template) == "" {
		req.Template = getDefaultTemplate(req.Category)
	}

	// è·å–è¿œç¨‹ ACL é…ç½®
	aclContent, err := fetchRemoteContent(req.RuleSource, req.UseProxy, req.ProxyLink)
	if err != nil {
		utils.FailWithMsg(c, "è·å–è¿œç¨‹é…ç½®å¤±è´¥: "+err.Error())
		return
	}

	// è§£æ ACL é…ç½®
	rulesets, proxyGroups := parseACLConfig(aclContent)

	// æ ¹æ®ç±»å‹ç”Ÿæˆé…ç½®
	var proxyGroupsStr, rulesStr string
	if req.Category == "surge" {
		proxyGroupsStr = generateSurgeProxyGroups(proxyGroups, req.EnableIncludeAll)
		rulesStr, err = generateSurgeRules(rulesets, req.Expand, req.UseProxy, req.ProxyLink)
	} else {
		proxyGroupsStr = generateClashProxyGroups(proxyGroups, req.EnableIncludeAll)
		rulesStr, err = generateClashRules(rulesets, req.Expand, req.UseProxy, req.ProxyLink)
	}

	if err != nil {
		utils.FailWithMsg(c, "ç”Ÿæˆè§„åˆ™å¤±è´¥: "+err.Error())
		return
	}

	// åˆå¹¶åˆ°æ¨¡æ¿å†…å®¹
	finalContent := mergeToTemplate(req.Template, proxyGroupsStr, rulesStr, req.Category)

	utils.OkDetailed(c, "ok", ConvertRulesResponse{
		Content: finalContent,
	})
}

// fetchRemoteContent è·å–è¿œç¨‹å†…å®¹
// æ”¯æŒä½¿ç”¨ä»£ç†èŠ‚ç‚¹ä¸‹è½½
func fetchRemoteContent(url string, useProxy bool, proxyLink string) (string, error) {
	data, err := utils.FetchWithProxy(url, useProxy, proxyLink, 30*time.Second, "")
	if err != nil {
		return "", err
	}
	return string(data), nil
}

// parseACLConfig è§£æ ACL é…ç½®
func parseACLConfig(content string) ([]ACLRuleset, []ACLProxyGroup) {
	var rulesets []ACLRuleset
	var proxyGroups []ACLProxyGroup

	scanner := bufio.NewScanner(strings.NewReader(content))
	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())

		// è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
		if line == "" || strings.HasPrefix(line, ";") || strings.HasPrefix(line, "#") {
			continue
		}

		// è§£æ ruleset=
		if strings.HasPrefix(line, "ruleset=") {
			parts := strings.SplitN(line[8:], ",", 2)
			if len(parts) == 2 {
				rulesets = append(rulesets, ACLRuleset{
					Group:   strings.TrimSpace(parts[0]),
					RuleURL: strings.TrimSpace(parts[1]),
				})
			}
		}

		// è§£æ custom_proxy_group=
		if strings.HasPrefix(line, "custom_proxy_group=") {
			pg := parseProxyGroup(line[19:])
			if pg.Name != "" {
				proxyGroups = append(proxyGroups, pg)
			}
		}
	}

	return rulesets, proxyGroups
}

// parseProxyGroup è§£æä»£ç†ç»„å®šä¹‰
// æ ¼å¼: name`type`proxy1`proxy2`...`url`interval,,tolerance
// æ”¯æŒè¯†åˆ«:
//   - .* é€šé…ç¬¦: åŒ¹é…æ‰€æœ‰èŠ‚ç‚¹ï¼Œç”Ÿæˆ include-all: true
//   - (æ¸¯|HK) æ­£åˆ™: åŒ¹é…ç‰¹å®šèŠ‚ç‚¹ï¼Œç”Ÿæˆ include-all: true + filter
//   - []ç»„å: ç­–ç•¥ç»„å¼•ç”¨ï¼Œå¦‚ []ğŸš€ èŠ‚ç‚¹é€‰æ‹©
func parseProxyGroup(line string) ACLProxyGroup {
	parts := strings.Split(line, "`")
	if len(parts) < 2 {
		return ACLProxyGroup{}
	}

	pg := ACLProxyGroup{
		Name:    parts[0],
		Type:    parts[1],
		Proxies: make([]string, 0),
	}

	// æ”¶é›†æ­£åˆ™è¿‡æ»¤å™¨
	var regexFilters []string

	for i := 2; i < len(parts); i++ {
		part := parts[i]

		// æ£€æµ‹æµ‹é€Ÿ URL
		if strings.HasPrefix(part, "http://") || strings.HasPrefix(part, "https://") {
			pg.URL = part
			continue
		}

		// æ£€æµ‹æ•°å­—æ ¼å¼ interval,,tolerance æˆ– interval
		if matched, _ := regexp.MatchString(`^\d+`, part); matched {
			// æ£€æŸ¥æ˜¯å¦æœ‰ ,, åˆ†éš”ç¬¦ (interval,,tolerance)
			if strings.Contains(part, ",") {
				numParts := strings.Split(part, ",")
				if len(numParts) >= 1 && numParts[0] != "" {
					fmt.Sscanf(numParts[0], "%d", &pg.Interval)
				}
				// tolerance åœ¨æœ€åä¸€ä¸ªéç©ºå…ƒç´ 
				for j := len(numParts) - 1; j >= 0; j-- {
					if numParts[j] != "" && j > 0 {
						fmt.Sscanf(numParts[j], "%d", &pg.Tolerance)
						break
					}
				}
			} else {
				fmt.Sscanf(part, "%d", &pg.Interval)
			}
			continue
		}

		// ä»£ç†åç§°ï¼Œå»æ‰ [] å‰ç¼€
		proxyName := part
		if strings.HasPrefix(part, "[]") {
			proxyName = part[2:]
		}

		// è·³è¿‡ç©ºå­—ç¬¦ä¸²
		if proxyName == "" {
			continue
		}

		// æ£€æµ‹ .* é€šé…ç¬¦: åŒ¹é…æ‰€æœ‰èŠ‚ç‚¹
		if proxyName == ".*" {
			pg.IncludeAll = true
			continue
		}

		// æ£€æµ‹æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼: (é€‰é¡¹1|é€‰é¡¹2|...)
		if isRegexProxyPattern(proxyName) {
			regexFilters = append(regexFilters, proxyName)
			continue
		}

		// æ™®é€šç­–ç•¥ç»„å¼•ç”¨
		pg.Proxies = append(pg.Proxies, proxyName)
	}

	// å¦‚æœæœ‰æ­£åˆ™è¿‡æ»¤å™¨ï¼Œè®¾ç½® IncludeAll å¹¶åˆå¹¶ filter
	if len(regexFilters) > 0 {
		pg.IncludeAll = true
		pg.Filter = mergeRegexFilters(regexFilters)
	}

	return pg
}

// generateClashProxyGroups ç”Ÿæˆ Clash æ ¼å¼çš„ä»£ç†ç»„
// æ”¯æŒ mihomo å†…æ ¸çš„ include-all + filter å‚æ•°
// enableIncludeAll: å¼ºåˆ¶ä¸ºæ‰€æœ‰ç»„å¯ç”¨ include-allï¼ˆè¦†ç›– ACL é…ç½®çš„æ™ºèƒ½æ£€æµ‹ï¼‰
func generateClashProxyGroups(groups []ACLProxyGroup, enableIncludeAll bool) string {
	var lines []string
	lines = append(lines, "proxy-groups:")

	for _, g := range groups {
		lines = append(lines, fmt.Sprintf("  - name: %s", g.Name))
		lines = append(lines, fmt.Sprintf("    type: %s", g.Type))

		if g.Type == "url-test" || g.Type == "fallback" {
			url := g.URL
			if url == "" {
				url = "http://www.gstatic.com/generate_204"
			}
			lines = append(lines, fmt.Sprintf("    url: %s", url))

			interval := g.Interval
			if interval <= 0 {
				interval = 300
			}
			lines = append(lines, fmt.Sprintf("    interval: %d", interval))

			tolerance := g.Tolerance
			if tolerance <= 0 {
				tolerance = 150
			}
			lines = append(lines, fmt.Sprintf("    tolerance: %d", tolerance))
		}

		// Include-All æ¨¡å¼é€»è¾‘ï¼š
		// - å¼€å¯æ¨¡å¼ (enableIncludeAll=true)ï¼šéœ€è¦åŒ…å«èŠ‚ç‚¹çš„ç»„ä½¿ç”¨ include-all + filterï¼Œå®¢æˆ·ç«¯è‡ªåŠ¨åŒ¹é…
		// - å…³é—­æ¨¡å¼ (enableIncludeAll=false)ï¼šproxies ç•™ç©ºï¼Œç”±ç³»ç»ŸæŒ‰é¡ºåºè¿½åŠ èŠ‚ç‚¹
		if g.IncludeAll && enableIncludeAll {
			// å¼€å¯æ¨¡å¼ï¼šä½¿ç”¨ include-all + filterï¼Œä¸éµå¾ªç³»ç»Ÿæ’åº
			lines = append(lines, "    include-all: true")
			if g.Filter != "" {
				lines = append(lines, fmt.Sprintf("    filter: %s", g.Filter))
			}
		}
		// å…³é—­æ¨¡å¼ï¼šä¸ç”Ÿæˆ include-allï¼Œproxies ä¸ºç©ºï¼Œç”± DecodeClash è¿½åŠ èŠ‚ç‚¹

		// è¾“å‡º proxiesï¼ˆç­–ç•¥ç»„å¼•ç”¨ï¼Œå¦‚ DIRECTã€å…¶ä»–ä»£ç†ç»„ç­‰ï¼‰
		if len(g.Proxies) > 0 {
			lines = append(lines, "    proxies:")
			for _, proxy := range g.Proxies {
				lines = append(lines, fmt.Sprintf("      - %s", proxy))
			}
		}
	}

	return strings.Join(lines, "\n")
}

// isRegexProxyPattern æ£€æµ‹æ˜¯å¦æ˜¯æ­£åˆ™ä»£ç†æ¨¡å¼
// æ ¼å¼: (é€‰é¡¹1|é€‰é¡¹2|é€‰é¡¹3)
func isRegexProxyPattern(proxy string) bool {
	proxy = strings.TrimSpace(proxy)
	if len(proxy) < 3 {
		return false
	}
	return strings.HasPrefix(proxy, "(") && strings.HasSuffix(proxy, ")") && strings.Contains(proxy, "|")
}

// mergeRegexFilters åˆå¹¶å¤šä¸ªæ­£åˆ™è¿‡æ»¤å™¨
// è¾“å…¥: ["(é¦™æ¸¯|HK)", "(æ—¥æœ¬|JP)"]
// è¾“å‡º: "(é¦™æ¸¯|HK|æ—¥æœ¬|JP)"
func mergeRegexFilters(filters []string) string {
	if len(filters) == 1 {
		return filters[0]
	}
	var allOptions []string
	for _, f := range filters {
		// å»é™¤é¦–å°¾æ‹¬å·ï¼Œæå–å†…éƒ¨é€‰é¡¹
		inner := strings.TrimPrefix(strings.TrimSuffix(f, ")"), "(")
		allOptions = append(allOptions, inner)
	}
	return "(" + strings.Join(allOptions, "|") + ")"
}

// generateClashRules ç”Ÿæˆ Clash æ ¼å¼çš„è§„åˆ™
func generateClashRules(rulesets []ACLRuleset, expand bool, useProxy bool, proxyLink string) (string, error) {
	var rules []string
	var providers []string // rule-providers
	providerIndex := make(map[string]bool)

	if expand {
		// å¹¶å‘è·å–æ‰€æœ‰è§„åˆ™åˆ—è¡¨
		rules = expandRulesParallel(rulesets, useProxy, proxyLink)
	} else {
		// ç”Ÿæˆ RULE-SET å¼•ç”¨ + rule-providers
		for _, rs := range rulesets {
			if strings.HasPrefix(rs.RuleURL, "[]") {
				// å†…è”è§„åˆ™
				rule := rs.RuleURL[2:] // å»æ‰ []
				if rule == "GEOIP,CN" {
					rules = append(rules, fmt.Sprintf("GEOIP,CN,%s", rs.Group))
				} else if rule == "FINAL" {
					rules = append(rules, fmt.Sprintf("MATCH,%s", rs.Group))
				} else if strings.HasPrefix(rule, "GEOIP,") {
					geo := strings.TrimPrefix(rule, "GEOIP,")
					rules = append(rules, fmt.Sprintf("GEOIP,%s,%s", geo, rs.Group))
				} else {
					rules = append(rules, fmt.Sprintf("%s,%s", rule, rs.Group))
				}
			} else if strings.HasPrefix(rs.RuleURL, "http") {
				// è¿œç¨‹è§„åˆ™ï¼Œè§£æå‡ºåç§°
				// ACL4SSR çš„ .list æ–‡ä»¶æ˜¯ classical ç±»å‹ï¼ŒåŒ…å«æ··åˆè§„åˆ™
				providerName, behavior := parseProviderInfo(rs.RuleURL)

				// æ·»åŠ  RULE-SET å¼•ç”¨
				rules = append(rules, fmt.Sprintf("RULE-SET,%s,%s", providerName, rs.Group))

				// æ·»åŠ  provider å®šä¹‰ï¼ˆé¿å…é‡å¤ï¼‰
				if !providerIndex[providerName] {
					providerIndex[providerName] = true
					providers = append(providers, generateProvider(providerName, rs.RuleURL, behavior, behavior))
				}
			}
		}
	}

	// ç”Ÿæˆ rules éƒ¨åˆ†
	var lines []string
	lines = append(lines, "rules:")
	for _, rule := range rules {
		// è·³è¿‡ Clash ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹ï¼ˆexpand æ¨¡å¼ä¸‹æ‰è¿‡æ»¤ RULE-SETï¼‰
		if isUnsupportedClashRule(rule, expand) {
			continue
		}
		lines = append(lines, fmt.Sprintf("  - %s", rule))
	}

	// å¦‚æœæœ‰ providersï¼Œæ·»åŠ  rule-providers éƒ¨åˆ†
	if len(providers) > 0 {
		lines = append(lines, "")
		lines = append(lines, "rule-providers:")
		for _, p := range providers {
			lines = append(lines, p)
		}
	}

	return strings.Join(lines, "\n"), nil
}

// parseProviderInfo ä» URL è§£æ provider åç§°å’Œè¡Œä¸ºç±»å‹
func parseProviderInfo(url string) (name string, behavior string) {
	// ä» URL æå–æ–‡ä»¶å
	parts := strings.Split(url, "/")
	filename := parts[len(parts)-1]

	// å»æ‰ .list æ‰©å±•å
	name = strings.TrimSuffix(filename, ".list")

	// é»˜è®¤è¡Œä¸ºç±»å‹
	behavior = "classical"

	return name, behavior
}

// generateProvider ç”Ÿæˆå•ä¸ª provider çš„ YAML
// ç”Ÿæˆ rule-providers é…ç½®ï¼Œä½¿ç”¨ text æ ¼å¼
func generateProvider(name, url, ruleType, behavior string) string {
	var lines []string
	lines = append(lines, fmt.Sprintf("  %s:", name))
	lines = append(lines, "    type: http")
	lines = append(lines, fmt.Sprintf("    behavior: %s", ruleType))
	lines = append(lines, fmt.Sprintf("    url: %s", url))
	lines = append(lines, "    format: text")
	lines = append(lines, "    path: ./providers/"+strings.ReplaceAll(name, " ", "_")+".txt")
	lines = append(lines, "    interval: 86400")
	return strings.Join(lines, "\n")
}

// expandRulesParallel å¹¶å‘å±•å¼€è§„åˆ™
func expandRulesParallel(rulesets []ACLRuleset, useProxy bool, proxyLink string) []string {
	type ruleResult struct {
		index int
		rules []string
	}

	results := make(chan ruleResult, len(rulesets))
	var wg sync.WaitGroup

	for i, rs := range rulesets {
		wg.Add(1)
		go func(idx int, ruleset ACLRuleset) {
			defer wg.Done()

			var rules []string
			if strings.HasPrefix(ruleset.RuleURL, "[]") {
				// å†…è”è§„åˆ™
				rule := ruleset.RuleURL[2:]
				if rule == "GEOIP,CN" {
					rules = append(rules, fmt.Sprintf("GEOIP,CN,%s", ruleset.Group))
				} else if rule == "FINAL" {
					rules = append(rules, fmt.Sprintf("MATCH,%s", ruleset.Group))
				} else if strings.HasPrefix(rule, "GEOIP,") {
					geo := strings.TrimPrefix(rule, "GEOIP,")
					rules = append(rules, fmt.Sprintf("GEOIP,%s,%s", geo, ruleset.Group))
				} else {
					rules = append(rules, fmt.Sprintf("%s,%s", rule, ruleset.Group))
				}
			} else if strings.HasPrefix(ruleset.RuleURL, "http") {
				// è·å–è¿œç¨‹è§„åˆ™
				content, err := fetchRemoteContent(ruleset.RuleURL, useProxy, proxyLink)
				if err != nil {
					utils.Error("è·å–è§„åˆ™å¤±è´¥ %s: %v", ruleset.RuleURL, err)
					results <- ruleResult{idx, rules}
					return
				}
				rules = parseRuleList(content, ruleset.Group)
			}
			results <- ruleResult{idx, rules}
		}(i, rs)
	}

	// ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
	go func() {
		wg.Wait()
		close(results)
	}()

	// æ”¶é›†ç»“æœå¹¶æŒ‰åŸé¡ºåºæ’åº
	orderedResults := make([][]string, len(rulesets))
	for r := range results {
		orderedResults[r.index] = r.rules
	}

	// åˆå¹¶ç»“æœ
	var allRules []string
	for _, rules := range orderedResults {
		allRules = append(allRules, rules...)
	}

	return allRules
}

// parseRuleList è§£æè§„åˆ™åˆ—è¡¨æ–‡ä»¶
// æ­£ç¡®å¤„ç† no-resolve å‚æ•°ä½ç½®ï¼šIP-CIDR,åœ°å€,ç­–ç•¥ç»„,no-resolve
func parseRuleList(content string, group string) []string {
	var rules []string
	scanner := bufio.NewScanner(strings.NewReader(content))

	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())

		// è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
		if line == "" || strings.HasPrefix(line, "#") {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦åŒ…å« no-resolve å‚æ•°
		// ACL4SSR æ ¼å¼: IP-CIDR,åœ°å€,no-resolve
		// Clash æ­£ç¡®æ ¼å¼: IP-CIDR,åœ°å€,ç­–ç•¥ç»„,no-resolve
		if strings.HasSuffix(line, ",no-resolve") {
			// ç§»é™¤æœ«å°¾çš„ no-resolveï¼Œæ·»åŠ ç­–ç•¥ç»„åå†åŠ å›å»
			lineWithoutNoResolve := strings.TrimSuffix(line, ",no-resolve")
			rules = append(rules, fmt.Sprintf("%s,%s,no-resolve", lineWithoutNoResolve, group))
		} else {
			// æ™®é€šè§„åˆ™ï¼Œç›´æ¥æ·»åŠ ç­–ç•¥ç»„
			rules = append(rules, fmt.Sprintf("%s,%s", line, group))
		}
	}

	return rules
}

// isUnsupportedClashRule æ£€æŸ¥æ˜¯å¦ä¸º Clash ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹
// Surge ç‰¹æœ‰çš„è§„åˆ™ç±»å‹åœ¨ Clash ä¸­ä¸å¯ç”¨ï¼Œéœ€è¦è¿‡æ»¤
// expand å‚æ•°æ§åˆ¶æ˜¯å¦è¿‡æ»¤ RULE-SETï¼ˆåªåœ¨å±•å¼€æ¨¡å¼ä¸‹è¿‡æ»¤ï¼‰
func isUnsupportedClashRule(rule string, expand bool) bool {
	// Clash ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹å‰ç¼€
	unsupportedPrefixes := []string{
		"URL-REGEX,",  // URL æ­£åˆ™åŒ¹é…
		"USER-AGENT,", // User-Agent åŒ¹é…
		//"PROCESS-NAME,", // è¿›ç¨‹ååŒ¹é…ï¼ˆéƒ¨åˆ† Clash ç‰ˆæœ¬ä¸æ”¯æŒï¼‰
		"DEST-PORT,", // ç›®æ ‡ç«¯å£ï¼ˆClash ä½¿ç”¨ DST-PORTï¼‰
		"SRC-PORT,",  // æºç«¯å£ï¼ˆClash ä½¿ç”¨ SRC-PORT ä½†æ ¼å¼å¯èƒ½ä¸åŒï¼‰
		"IN-PORT,",   // å…¥ç«™ç«¯å£
		"PROTOCOL,",  // åè®®åŒ¹é…
		"SCRIPT,",    // è„šæœ¬è§„åˆ™
		"SUBNET,",    // å­ç½‘åŒ¹é…
	}

	// RULE-SET åªåœ¨å±•å¼€æ¨¡å¼ä¸‹è¿‡æ»¤ï¼ˆå±•å¼€åä¸åº”æœ‰ RULE-SET å¼•ç”¨ï¼‰
	if expand {
		unsupportedPrefixes = append(unsupportedPrefixes, "RULE-SET,")
	}

	for _, prefix := range unsupportedPrefixes {
		if strings.HasPrefix(rule, prefix) {
			return true
		}
	}
	return false
}

// generateSurgeProxyGroups ç”Ÿæˆ Surge æ ¼å¼çš„ä»£ç†ç»„
// æ”¯æŒ policy-regex-filter å’Œ include-all-proxies å‚æ•°
// enableIncludeAll: æ˜¯å¦ä½¿ç”¨ include-all-proxies æ¨¡å¼ï¼ˆå¼€å¯ä¸éµå¾ªç³»ç»Ÿæ’åºï¼Œå…³é—­ç”±ç³»ç»Ÿè¿½åŠ èŠ‚ç‚¹ï¼‰
func generateSurgeProxyGroups(groups []ACLProxyGroup, enableIncludeAll bool) string {
	var lines []string
	lines = append(lines, "[Proxy Group]")

	for _, g := range groups {
		var line string
		proxies := g.Proxies
		proxiesStr := ""
		if len(proxies) > 0 {
			proxiesStr = strings.Join(proxies, ", ")
		}

		// æå– Surge æ ¼å¼çš„ filterï¼ˆå»é™¤æ‹¬å·ï¼‰
		surgeFilter := ""
		if g.Filter != "" {
			surgeFilter = strings.TrimPrefix(strings.TrimSuffix(g.Filter, ")"), "(")
		}

		// Include-All æ¨¡å¼é€»è¾‘ï¼š
		// - å¼€å¯æ¨¡å¼ï¼šéœ€è¦åŒ…å«èŠ‚ç‚¹çš„ç»„ä½¿ç”¨ include-all-proxies + filter
		// - å…³é—­æ¨¡å¼ï¼šproxies ç•™ç©ºï¼Œç”± DecodeSurge è¿½åŠ èŠ‚ç‚¹
		useIncludeAll := g.IncludeAll && enableIncludeAll

		if g.Type == "url-test" || g.Type == "fallback" {
			url := g.URL
			if url == "" {
				url = "http://www.gstatic.com/generate_204"
			}
			interval := g.Interval
			if interval <= 0 {
				interval = 300
			}
			tolerance := g.Tolerance
			if tolerance <= 0 {
				tolerance = 150
			}

			if useIncludeAll && g.Filter != "" {
				// å¼€å¯æ¨¡å¼ + æœ‰æ­£åˆ™è¿‡æ»¤å™¨
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s, url=%s, interval=%d, timeout=5, tolerance=%d, policy-regex-filter=%s, include-all-proxies=1",
						g.Name, g.Type, proxiesStr, url, interval, tolerance, surgeFilter)
				} else {
					line = fmt.Sprintf("%s = %s, url=%s, interval=%d, timeout=5, tolerance=%d, policy-regex-filter=%s, include-all-proxies=1",
						g.Name, g.Type, url, interval, tolerance, surgeFilter)
				}
			} else if useIncludeAll {
				// å¼€å¯æ¨¡å¼ + .* é€šé…ç¬¦
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s, url=%s, interval=%d, timeout=5, tolerance=%d, include-all-proxies=1",
						g.Name, g.Type, proxiesStr, url, interval, tolerance)
				} else {
					line = fmt.Sprintf("%s = %s, url=%s, interval=%d, timeout=5, tolerance=%d, include-all-proxies=1",
						g.Name, g.Type, url, interval, tolerance)
				}
			} else {
				// å…³é—­æ¨¡å¼ï¼šä¸æ·»åŠ  include-all-proxiesï¼Œç”± DecodeSurge è¿½åŠ èŠ‚ç‚¹
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s, url=%s, interval=%d, timeout=5, tolerance=%d",
						g.Name, g.Type, proxiesStr, url, interval, tolerance)
				} else {
					// proxies ä¸ºç©ºï¼ŒDecodeSurge ä¼šè¿½åŠ èŠ‚ç‚¹
					line = fmt.Sprintf("%s = %s, url=%s, interval=%d, timeout=5, tolerance=%d",
						g.Name, g.Type, url, interval, tolerance)
				}
			}
		} else {
			// select, load-balance ç­‰ç±»å‹
			if useIncludeAll && g.Filter != "" {
				// å¼€å¯æ¨¡å¼ + æœ‰æ­£åˆ™è¿‡æ»¤å™¨
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s, policy-regex-filter=%s, include-all-proxies=1",
						g.Name, g.Type, proxiesStr, surgeFilter)
				} else {
					line = fmt.Sprintf("%s = %s, policy-regex-filter=%s, include-all-proxies=1",
						g.Name, g.Type, surgeFilter)
				}
			} else if useIncludeAll {
				// å¼€å¯æ¨¡å¼ + .* é€šé…ç¬¦
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s, include-all-proxies=1", g.Name, g.Type, proxiesStr)
				} else {
					line = fmt.Sprintf("%s = %s, include-all-proxies=1", g.Name, g.Type)
				}
			} else {
				// å…³é—­æ¨¡å¼ï¼šä¸æ·»åŠ  include-all-proxies
				if proxiesStr != "" {
					line = fmt.Sprintf("%s = %s, %s", g.Name, g.Type, proxiesStr)
				} else {
					// proxies ä¸ºç©ºï¼ŒDecodeSurge ä¼šè¿½åŠ èŠ‚ç‚¹
					line = fmt.Sprintf("%s = %s", g.Name, g.Type)
				}
			}
		}
		lines = append(lines, line)
	}

	return strings.Join(lines, "\n")
}

// extractSurgeRegexFilter ä»æ­£åˆ™æ¨¡å¼åˆ—è¡¨ä¸­æå– Surge æ ¼å¼çš„ filter
// è¾“å…¥: ["(é¦™æ¸¯|HK)", "(æ—¥æœ¬|JP)"]
// è¾“å‡º: "é¦™æ¸¯|HK|æ—¥æœ¬|JP"
func extractSurgeRegexFilter(filters []string) string {
	var allOptions []string
	for _, f := range filters {
		// å»é™¤é¦–å°¾æ‹¬å·ï¼Œæå–å†…éƒ¨é€‰é¡¹
		inner := strings.TrimPrefix(strings.TrimSuffix(f, ")"), "(")
		allOptions = append(allOptions, inner)
	}
	return strings.Join(allOptions, "|")
}

// generateSurgeRules ç”Ÿæˆ Surge æ ¼å¼çš„è§„åˆ™
func generateSurgeRules(rulesets []ACLRuleset, expand bool, useProxy bool, proxyLink string) (string, error) {
	var lines []string
	lines = append(lines, "[Rule]")

	if expand {
		// å±•å¼€è§„åˆ™
		rules := expandRulesParallel(rulesets, useProxy, proxyLink)
		for _, rule := range rules {
			// è½¬æ¢ Clash æ ¼å¼åˆ° Surge æ ¼å¼
			// MATCH -> FINAL
			if strings.HasPrefix(rule, "MATCH,") {
				rule = "FINAL," + strings.TrimPrefix(rule, "MATCH,")
			}
			lines = append(lines, rule)
		}
	} else {
		// ç”Ÿæˆ RULE-SET å¼•ç”¨
		for _, rs := range rulesets {
			if strings.HasPrefix(rs.RuleURL, "[]") {
				rule := rs.RuleURL[2:]
				if rule == "GEOIP,CN" {
					lines = append(lines, fmt.Sprintf("GEOIP,CN,%s", rs.Group))
				} else if rule == "FINAL" {
					lines = append(lines, fmt.Sprintf("FINAL,%s", rs.Group))
				} else {
					lines = append(lines, fmt.Sprintf("%s,%s", rule, rs.Group))
				}
			} else if strings.HasPrefix(rs.RuleURL, "http") {
				lines = append(lines, fmt.Sprintf("RULE-SET,%s,%s,update-interval=86400", rs.RuleURL, rs.Group))
			}
		}
	}

	return strings.Join(lines, "\n"), nil
}

// mergeToTemplate å°†ç”Ÿæˆçš„ä»£ç†ç»„å’Œè§„åˆ™åˆå¹¶åˆ°æ¨¡æ¿å†…å®¹ä¸­
func mergeToTemplate(template, proxyGroups, rules, category string) string {
	if category == "surge" {
		return mergeSurgeTemplate(template, proxyGroups, rules)
	}
	return mergeClashTemplate(template, proxyGroups, rules)
}

// mergeClashTemplate åˆå¹¶ Clash æ¨¡æ¿
// ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢æ–¹å¼ï¼Œé¿å… yaml.Marshal è½¬ä¹‰ emoji
func mergeClashTemplate(template, proxyGroups, rules string) string {
	if strings.TrimSpace(template) == "" {
		// æ¨¡æ¿ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç”Ÿæˆçš„å†…å®¹
		return proxyGroups + "\n\n" + rules
	}

	lines := strings.Split(template, "\n")
	var result []string
	skipSection := ""
	sectionsToReplace := map[string]bool{
		"proxy-groups:":   true,
		"rules:":          true,
		"rule-providers:": true,
	}

	for i, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// æ£€æŸ¥æ˜¯å¦è¿›å…¥éœ€è¦æ›¿æ¢çš„ section
		if sectionsToReplace[trimmedLine] {
			skipSection = trimmedLine
			continue
		}

		// å¦‚æœå½“å‰åœ¨éœ€è¦è·³è¿‡çš„ section ä¸­
		if skipSection != "" {
			// æ£€æŸ¥æ˜¯å¦åˆ°äº†æ–°çš„é¡¶çº§ keyï¼ˆä¸ä»¥ç©ºæ ¼å¼€å¤´ä¸”ä»¥ : ç»“å°¾ï¼‰
			if trimmedLine != "" && !strings.HasPrefix(line, " ") && !strings.HasPrefix(line, "\t") {
				// æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯åˆ—è¡¨æˆ–åµŒå¥—å†…å®¹
				if strings.HasSuffix(trimmedLine, ":") || (i+1 < len(lines) && strings.HasPrefix(strings.TrimSpace(lines[i+1]), "-")) {
					skipSection = ""
					result = append(result, line)
					continue
				}
				skipSection = ""
				result = append(result, line)
				continue
			}
			// ä»åœ¨éœ€è¦è·³è¿‡çš„ section ä¸­ï¼Œè·³è¿‡æ­¤è¡Œ
			continue
		}

		result = append(result, line)
	}

	// ç»„åˆç»“æœ
	resultStr := strings.Join(result, "\n")
	resultStr = strings.TrimRight(resultStr, "\n")

	// æ·»åŠ ç”Ÿæˆçš„ä»£ç†ç»„å’Œè§„åˆ™
	resultStr += "\n\n" + proxyGroups + "\n\n" + rules

	return resultStr
}

// mergeSurgeTemplate åˆå¹¶ Surge æ¨¡æ¿
func mergeSurgeTemplate(template, proxyGroups, rules string) string {
	lines := strings.Split(template, "\n")
	var result []string

	skipSection := ""
	sectionsToReplace := map[string]bool{
		"[Proxy Group]": true,
		"[Rule]":        true,
	}

	for _, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// æ£€æŸ¥æ˜¯å¦è¿›å…¥éœ€è¦æ›¿æ¢çš„ section
		if strings.HasPrefix(trimmedLine, "[") && strings.HasSuffix(trimmedLine, "]") {
			if sectionsToReplace[trimmedLine] {
				skipSection = trimmedLine
				continue
			} else {
				skipSection = ""
			}
		}

		// è·³è¿‡éœ€è¦æ›¿æ¢çš„ section çš„å†…å®¹
		if skipSection != "" {
			continue
		}

		result = append(result, line)
	}

	// æ·»åŠ ç”Ÿæˆçš„å†…å®¹
	resultStr := strings.Join(result, "\n")
	resultStr = strings.TrimRight(resultStr, "\n")
	resultStr += "\n\n" + proxyGroups + "\n\n" + rules

	return resultStr
}

// detectTemplateType æ£€æµ‹æ¨¡æ¿ç±»å‹
func detectTemplateType(template string) string {
	if strings.TrimSpace(template) == "" {
		return ""
	}

	// Surge ç‰¹å¾: [General], [Proxy], [Proxy Group], [Rule] sections
	surgePatterns := []string{"[General]", "[Proxy]", "[Proxy Group]", "[Rule]"}
	for _, pattern := range surgePatterns {
		if strings.Contains(template, pattern) {
			return "surge"
		}
	}

	// Clash ç‰¹å¾: YAML æ ¼å¼ï¼ŒåŒ…å« port:, proxies:, proxy-groups:, rules:
	clashPatterns := []string{"port:", "proxies:", "proxy-groups:", "rules:", "socks-port:", "dns:", "mode:"}
	for _, pattern := range clashPatterns {
		if strings.Contains(template, pattern) {
			return "clash"
		}
	}

	return ""
}

// getDefaultTemplate è·å–é»˜è®¤æ¨¡æ¿å†…å®¹
// ä¼˜å…ˆä»ç³»ç»Ÿè®¾ç½®è¯»å–ï¼Œå¦‚æœæœªé…ç½®åˆ™è¿”å›ç¡¬ç¼–ç é»˜è®¤å€¼
func getDefaultTemplate(category string) string {
	settingKey := "base_template_" + category
	template, err := models.GetSetting(settingKey)
	if err == nil && strings.TrimSpace(template) != "" {
		return template
	}

	// å›é€€åˆ°ç¡¬ç¼–ç é»˜è®¤å€¼
	if category == "surge" {
		return `[General]
loglevel = notify
bypass-system = true
skip-proxy = 127.0.0.1,192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,100.64.0.0/10,localhost,*.local,e.crashlytics.com,captive.apple.com,::ffff:0:0:0:0/1,::ffff:128:0:0:0/1
bypass-tun = 192.168.0.0/16,10.0.0.0/8,172.16.0.0/12
dns-server = 119.29.29.29,223.5.5.5,218.30.19.40,61.134.1.4
external-controller-access = password@0.0.0.0:6170
http-api = password@0.0.0.0:6171
test-timeout = 5
http-api-web-dashboard = true
exclude-simple-hostnames = true
allow-wifi-access = true
http-listen = 0.0.0.0:6152
socks5-listen = 0.0.0.0:6153
wifi-access-http-port = 6152
wifi-access-socks5-port = 6153

[Proxy]
DIRECT = direct

`
	}

	// Clash é»˜è®¤æ¨¡æ¿
	return `port: 7890
socks-port: 7891
allow-lan: true
mode: Rule
log-level: info
external-controller: :9090
dns:
  enabled: true
  nameserver:
    - 119.29.29.29
    - 223.5.5.5
  fallback:
    - 8.8.8.8
    - 8.8.4.4
    - tls://1.0.0.1:853
    - tls://dns.google:853
proxies: ~

`
}
